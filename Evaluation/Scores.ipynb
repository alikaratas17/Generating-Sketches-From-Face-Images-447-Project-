{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scores.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kFovQPo2XcTT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import trange, tqdm_notebook\n",
        "import copy\n",
        "from torch.distributions.uniform import Uniform\n",
        "from torch.distributions.normal import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from scipy.linalg import sqrtm\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class I(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
        "    self.model.eval()\n",
        "\n",
        "    #self.model = timm.create_model('inception_v3', pretrained=True, features_only=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #https://discuss.pytorch.org/t/extract-the-2048-vector-of-a-fine-tuned-inception-v3-on-test-set/152510/4 \n",
        "    activation = {}\n",
        "    def get_activation(name):\n",
        "        def hook(model, input, output):\n",
        "            activation[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    self.model.avgpool.register_forward_hook(get_activation(\"avgpool\"))\n",
        "\n",
        "    out = self.model(x)\n",
        "\n",
        "    return activation['avgpool'].squeeze(3).squeeze(2)\n"
      ],
      "metadata": {
        "id": "XznGBXEhXsez"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FID(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.inception = I()\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    with torch.no_grad():\n",
        "      features_x = self.inception(x).detach().cpu().numpy()\n",
        "      features_y = self.inception(y).detach().cpu().numpy()\n",
        "      #print(features_x)\n",
        "      #print(features_y)\n",
        "      mu_x = features_x.mean(axis=0)\n",
        "      mu_y = features_y.mean(axis=0)\n",
        "      mu_part = np.dot(mu_x - mu_y, mu_x - mu_y)\n",
        "      #print(mu_part)\n",
        "      covariance_x = np.cov(features_x, rowvar=False)\n",
        "      covariance_y = np.cov(features_y, rowvar=False)\n",
        "      #print(covariance_x)\n",
        "      #print(covariance_y)\n",
        "      #print(np.multiply(covariance_x, covariance_y).shape)\n",
        "      trace = covariance_x + covariance_y - 2 * sqrtm(np.dot(covariance_x, covariance_y))\n",
        "      trace = np.trace(trace.real)\n",
        "      #print(trace)\n",
        "      fid = mu_part - trace\n",
        "      #print(f\"real fid: {fid}\")\n",
        "      #return features_x, features_y\n",
        "      return fid\n"
      ],
      "metadata": {
        "id": "9xbWsUkxY_7j"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fid_experiment():\n",
        "  toTensorTransform = transforms.ToTensor()\n",
        "  sample_number_1 = 2 #TODO correct this\n",
        "  sample_number_2 = 2 #TODO correct this\n",
        "\n",
        "  inputs1 = torch.zeros([sample_number_1, 3, 256, 256]) #TODO fix size\n",
        "  for i in range(1, sample_number_1 + 1):\n",
        "    img = Image.open(\"generated_sketch_\" + str(i) + \".jpg\")\n",
        "    tensor = toTensorTransform(img)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    inputs1[i] = tensor\n",
        "  inputs1 = inputs1.cuda()\n",
        "\n",
        "  inputs2 = torch.zeros([sample_number_2, 3, 256, 256]) #TODO fix size\n",
        "  for i in range(1, sample_number_2 + 1):\n",
        "    img = Image.open(\"true_sketch_\" + str(i) + \".jpg\")\n",
        "    tensor = toTensorTransform(img)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    inputs2[i] = tensor\n",
        "  inputs2 = inputs2.cuda()\n",
        "\n",
        "  fid = FID().cuda()\n",
        "  fid_val = fid(inputs1, inputs2)\n",
        "\n",
        "  return fid_val\n"
      ],
      "metadata": {
        "id": "vABo0Oy8Xu1K"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fid_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I4t1KDeKGzw",
        "outputId": "f1192a36-4839-4bc6-d9e9-eed757223aa4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0002300919490065837"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition\n",
        "#https://github.com/ageitgey/face_recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBCm4OogAxWu",
        "outputId": "2d36711f-97b3-4983-df05-b53f4a7d33bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 100.1 MB 23 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.21.6)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0+zzzcolab20220513001918)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566186 sha256=2c4cced1b4f2bf265ba3309954f2c713d69904110e7637685086e5524e1bf8e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import face_recognition\n",
        "\n",
        "def face_recognition_experiment():\n",
        "  #I assume that known_sample_i corresponds to test_sample_i\n",
        "  known_faces = []\n",
        "  #TODO: how many samples exist? assign to sample_number\n",
        "  sample_number = 3\n",
        "  for i in range(1, sample_number + 1):\n",
        "    image = face_recognition.load_image_file(\"known_sample_\" + str(i) + \".jpg\")\n",
        "    face_encoding = face_recognition.face_encodings(image)[0]\n",
        "    known_faces.append(face_encoding)\n",
        "\n",
        "  number_of_accurates = 0\n",
        "  for i in range(1, sample_number + 1):\n",
        "    unknown_image = face_recognition.load_image_file(\"test_sample_\" + str(i) + \".jpg\")\n",
        "    unknown_face_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
        "    results = face_recognition.compare_faces(known_faces, unknown_face_encoding)\n",
        "    \n",
        "    accurate = True\n",
        "    for j, result in enumerate(results):\n",
        "      accurate = accurate and ((j == i) == result)\n",
        "      accurate = accurate and ((j != i) != result)\n",
        "    #print(accurate)\n",
        "    if accurate:\n",
        "      number_of_accurates += 1 \n",
        "\n",
        "  return number_of_accurates / sample_number\n"
      ],
      "metadata": {
        "id": "0I0yLp0mBQRF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_recognition_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV--FO_zE1mq",
        "outputId": "10bb4976-6ee0-4b8f-d512-3a1ef11ae041"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}